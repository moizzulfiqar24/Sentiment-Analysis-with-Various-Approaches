{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Loading and Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Paths to the CSV files\n",
    "train_csv_path = 'Data/train.csv'  \n",
    "test_csv_path = 'Data/test.csv'    \n",
    "\n",
    "# Read the datasets\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "reviews_train = train_df['review'].values\n",
    "labels_train = train_df['sentiment'].values\n",
    "reviews_test = test_df['review'].values\n",
    "labels_test = test_df['sentiment'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Text Preprocessing and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn, wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank POS tags to WordNet POS tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_sentiwordnet_score(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentiment_score = 0\n",
    "    tokens = word_tokenize(review)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    for word, tag in tagged:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            continue\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "        sentiment_score += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "    return sentiment_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Feature Engineering and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(reviews_train)\n",
    "\n",
    "# Calculate sentiment scores\n",
    "sentiment_scores = np.array([calculate_sentiwordnet_score(review) for review in reviews_train]).reshape(-1, 1)\n",
    "\n",
    "# Augment TF-IDF features with sentiment scores\n",
    "X_augmented = np.hstack((X_tfidf.toarray(), sentiment_scores))\n",
    "X_augmented = normalize(X_augmented)  # Normalize features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.0431196689605713 seconds\n",
      "Accuracy: 0.8128333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdullah Maqsood\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_augmented, labels_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the training time\n",
    "training_time = end_time - start_time\n",
    "print(f'Training time: {training_time} seconds')\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = lr_model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, predictions)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
