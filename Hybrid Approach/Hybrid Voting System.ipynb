{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Setup and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "train_csv_path = 'Data/train.csv'\n",
    "test_csv_path = 'Data/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "reviews_train = train_df['review'].values\n",
    "sentiments_train = train_df['sentiment'].values\n",
    "reviews_test = test_df['review'].values\n",
    "sentiments_test = test_df['sentiment'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define Lexicon-based Sentiment Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to C:\\Users\\Abdullah\n",
      "[nltk_data]     Maqsood\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Abdullah\n",
      "[nltk_data]     Maqsood\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Abdullah Maqsood\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Abdullah\n",
      "[nltk_data]     Maqsood\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from afinn import Afinn\n",
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Helper function to map NLTK's part-of-speech tags to WordNet's part-of-speech tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define function for SentiWordNet\n",
    "def sentiwordnet_score(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentiment_score = 0\n",
    "    \n",
    "    # Tokenize and POS tag the review\n",
    "    tokens = word_tokenize(review)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    for word, tag in tagged:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            continue\n",
    "        \n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "        \n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            continue\n",
    "        \n",
    "        # Take the first sense, the most common\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "        \n",
    "        sentiment_score += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "    \n",
    "    # Return 'positive' if the sentiment score is positive, else 'negative'\n",
    "    return 'positive' if sentiment_score > 0 else 'negative'\n",
    "\n",
    "# Define function for AFINN\n",
    "afinn = Afinn()\n",
    "def afinn_score(review):\n",
    "    return 'positive' if afinn.score(review) > 0 else 'negative'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Preprocess Data for ML Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train = vectorizer.fit_transform(reviews_train)\n",
    "X_test = vectorizer.transform(reviews_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train Machine Learning Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize models\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Record the start time \n",
    "start_time = time.time()\n",
    "\n",
    "# Fit models\n",
    "gb_model.fit(X_train, sentiments_train)\n",
    "dt_model.fit(X_train, sentiments_train)\n",
    "knn_model.fit(X_train, sentiments_train)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Combine Predictions and Vote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping between sentiment labels and numeric values\n",
    "sentiment_mapping = {'positive': 1, 'negative': 0}\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Get predictions from ML models\n",
    "predictions_gb = gb_model.predict(X_test)\n",
    "predictions_dt = dt_model.predict(X_test)\n",
    "predictions_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Apply lexicon-based methods\n",
    "predictions_swn = np.array([sentiwordnet_score(review) for review in reviews_test])\n",
    "predictions_afinn = np.array([afinn_score(review) for review in reviews_test])\n",
    "\n",
    "# Convert predictions from lexicon-based methods to numeric\n",
    "predictions_swn_numeric = np.array([sentiment_mapping[pred] for pred in predictions_swn])\n",
    "predictions_afinn_numeric = np.array([sentiment_mapping[pred] for pred in predictions_afinn])\n",
    "\n",
    "# Convert predictions from ML models to numeric\n",
    "predictions_gb_numeric = np.array([sentiment_mapping[pred] for pred in predictions_gb])\n",
    "predictions_dt_numeric = np.array([sentiment_mapping[pred] for pred in predictions_dt])\n",
    "predictions_knn_numeric = np.array([sentiment_mapping[pred] for pred in predictions_knn])\n",
    "\n",
    "# Combine numeric predictions\n",
    "combined_predictions_numeric = np.vstack((predictions_gb_numeric, predictions_dt_numeric, predictions_knn_numeric, predictions_swn_numeric, predictions_afinn_numeric))\n",
    "\n",
    "# Determine the mode (most common prediction) across methods for each review\n",
    "final_predictions_numeric, _ = mode(combined_predictions_numeric, axis=0)\n",
    "\n",
    "# Reverse the mapping to convert numeric predictions back to string labels\n",
    "reverse_sentiment_mapping = {v: k for k, v in sentiment_mapping.items()}\n",
    "\n",
    "# Convert numeric predictions back to string labels\n",
    "final_predictions = np.array([reverse_sentiment_mapping[pred] for pred in final_predictions_numeric.flatten()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Evaluate Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 258.66820001602173 seconds\n",
      "Hybrid model accuracy: 0.7876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate and print the training time\n",
    "training_time = end_time - start_time\n",
    "print(f'Training time: {training_time} seconds')\n",
    "\n",
    "accuracy = accuracy_score(sentiments_test, final_predictions)\n",
    "print(f'Hybrid model accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
